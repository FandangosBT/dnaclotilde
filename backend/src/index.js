import express from 'express'
import cors from 'cors'
import pino from 'pino'
import pinoHttp from 'pino-http'
import { z } from 'zod'
import { config } from './config.js'
import { streamChat } from './llm/openaiClient.js'
import { AppError, ERROR_CODES, mapUpstreamError } from './utils/errors.js'

const app = express()
const logger = pino({ level: process.env.LOG_LEVEL || 'info' })

app.use(cors({ origin: config.corsOrigin }))
// Responder automaticamente preflight (OPTIONS)
app.options('*', cors({ origin: config.corsOrigin }))
app.use(express.json({ limit: '10mb' }))
app.use(pinoHttp({ logger }))

// Rate limiting in-memory por IP (janela fixa)
const WINDOW_MS = 5 * 60 * 1000 // 5 minutos
const LIMITS = { general: 60, chat: 30 }
const buckets = new Map() // ip -> { start: number, countGeneral: number, countChat: number }
function rateLimit(kind = 'general') {
  return (req, res, next) => {
    const ip =
      req.headers['x-forwarded-for']?.toString().split(',')[0]?.trim() ||
      req.socket.remoteAddress ||
      'unknown'
    const now = Date.now()
    let b = buckets.get(ip)
    if (!b || now - b.start > WINDOW_MS) {
      b = { start: now, countGeneral: 0, countChat: 0 }
      buckets.set(ip, b)
    }
    if (kind === 'chat') b.countChat++
    else b.countGeneral++
    const limit = kind === 'chat' ? LIMITS.chat : LIMITS.general
    const used = kind === 'chat' ? b.countChat : b.countGeneral
    if (used > limit) {
      res.status(429).json({
        code: 'RATE_LIMITED',
        message: 'Muitas requisi√ß√µes. Tente novamente em alguns minutos.',
      })
      return
    }
    next()
  }
}

app.get('/health', rateLimit('general'), (req, res) => {
  res.json({ status: 'ok' })
})

const ChatSchema = z.object({
  message: z.string().min(1),
  mode: z.enum(['SDR', 'Closer']).default('SDR'),
  tone: z.enum(['breve', 'detalhado']).default('breve'),
  formality: z.enum(['informal', 'formal']).default('informal'),
  objective: z
    .enum(['qualificar', 'obje√ß√µes', 'descoberta', 'fechamento', 'follow-up'])
    .default('qualificar'),
  attachments: z
    .array(
      z.object({
        kind: z.enum(['image', 'text']),
        content: z.string().min(1),
        mime: z.string().optional(),
        name: z.string().optional(),
      }),
    )
    .optional()
    .default([]),
})

// Sugest√µes est√°ticas por objetivo/modo (v1 simples, sem chamada extra ao LLM)
function generateSuggestions(objective = 'qualificar', mode = 'SDR') {
  const base = {
    qualificar: [
      'Perguntar autoridade e papel na decis√£o',
      'Checar or√ßamento aproximado',
      'Sugerir call de 15 minutos esta semana',
    ],
    obje√ß√µes: [
      'Explorar obje√ß√£o de pre√ßo com foco em ROI',
      'Trazer um case curto de cliente similar',
      'Oferecer um piloto/POC enxuto',
    ],
    descoberta: [
      'Aprofundar nos crit√©rios de sucesso',
      'Entender integra√ß√£o com sistemas atuais',
      'Validar prazos e partes envolvidas',
    ],
    fechamento: [
      'Alinhar pr√≥ximos passos e respons√°veis',
      'Solicitar aprova√ß√£o para contrato',
      'Confirmar cronograma de onboarding',
    ],
    'follow-up': [
      'Enviar resumo com pr√≥ximos passos',
      'Agendar retorno em data espec√≠fica',
      'Perguntar bloqueios para avan√ßar',
    ],
  }
  const list = base[objective] || base.qualificar
  // Varia√ß√£o leve por modo
  if (mode === 'Closer' && objective === 'qualificar') {
    return [
      'Validar decisores do comit√™',
      'Confirmar crit√©rios de compra',
      'Propor call com sponsor',
    ]
  }
  return list.slice(0, 3)
}

// SSE endpoint
app.post('/chat/stream', rateLimit('chat'), async (req, res) => {
  res.setHeader('Content-Type', 'text/event-stream')
  res.setHeader('Cache-Control', 'no-cache')
  res.setHeader('Connection', 'keep-alive')

  // For√ßa envio imediato dos headers
  if (typeof res.flushHeaders === 'function') {
    try {
      res.flushHeaders()
    } catch (_) {}
  }

  // Envia um primeiro evento "open" com data para evitar fechamento por buffers de browser/proxies
  try {
    res.write('event: open\n')
    res.write('data: {}\n\n')
  } catch (_) {}

  // Heartbeat para manter conex√£o viva
  const heartbeat = setInterval(() => {
    try {
      res.write('event: ping\n')
      res.write('data: \n\n')
    } catch (_) {}
  }, 15000)

  // Cancelamento/cleanup quando a resposta √© fechada (cliente desconectou ou servidor finalizou)
  const controller = new AbortController()
  const startAt = Date.now()
  const onResClose = () => {
    clearInterval(heartbeat)
    try {
      controller.abort(new Error('Client disconnected'))
    } catch (_) {}
    if ((process.env.LOG_LEVEL || 'info') === 'debug' && process.env.NODE_ENV !== 'production') {
      req.log?.info(
        { elapsedMs: Date.now() - startAt, headersSent: res.headersSent },
        'response close (cleanup)',
      )
    }
    // N√£o tente escrever no response aqui ‚Äî ele j√° est√° fechado
  }
  res.on('close', onResClose)

  // Logs e tratamento para abort expl√≠cito do request (antes da resposta fechar)
  req.on?.('aborted', () => {
    clearInterval(heartbeat)
    try {
      controller.abort(new Error('Request aborted'))
    } catch (_) {}
    if ((process.env.LOG_LEVEL || 'info') === 'debug' && process.env.NODE_ENV !== 'production') {
      req.log?.warn({ elapsedMs: Date.now() - startAt }, 'request aborted event')
    }
  })
  res.on?.('close', () => {
    if ((process.env.LOG_LEVEL || 'info') === 'debug' && process.env.NODE_ENV !== 'production') {
      req.log?.warn(
        { elapsedMs: Date.now() - startAt, headersSent: res.headersSent },
        'response close event',
      )
    }
  })

  let parsed
  try {
    parsed = ChatSchema.parse(req.body)
  } catch (e) {
    clearInterval(heartbeat)
    res.write('event: error\n')
    res.write(`data: ${JSON.stringify({ code: 'BAD_REQUEST', message: 'Payload inv√°lido' })}\n\n`)
    try {
      res.write('event: end\n')
    } catch (_) {}
    try {
      res.write('data: {}\n\n')
    } catch (_) {}
    res.end()
    return
  }

  if (!config.openai.apiKey) {
    clearInterval(heartbeat)
    res.write('event: error\n')
    res.write(
      `data: ${JSON.stringify({ code: 'MISSING_API_KEY', message: 'OPENAI_API_KEY n√£o configurada' })}\n\n`,
    )
    res.end()
    return
  }

  let hadChunks = false
  try {
    if ((process.env.LOG_LEVEL || 'info') === 'debug' && process.env.NODE_ENV !== 'production') {
      req.log?.info(
        { model: config.openai.modelPreferred || config.openai.model },
        'streamChat start',
      )
    }

    await streamChat(
      parsed,
      (chunk) => {
        hadChunks = true
        res.write(`data: ${JSON.stringify({ chunk })}\n\n`)
      },
      { signal: controller.signal, timeoutMs: 90000 },
    )

    if (hadChunks) {
      const suggestions = generateSuggestions(parsed.objective, parsed.mode)
      if (suggestions?.length) {
        res.write('event: suggestions\n')
        res.write(`data: ${JSON.stringify({ suggestions })}\n\n`)
      }
    }
  } catch (err) {
    const mapped = err instanceof AppError ? err : mapUpstreamError(err, 'OpenAI')
    req.log?.error({ err: mapped, elapsedMs: Date.now() - startAt }, 'streamChat error')
    res.write('event: error\n')
    res.write(`data: ${JSON.stringify(mapped.toJSON())}\n\n`)
  } finally {
    clearInterval(heartbeat)
    try {
      res.write('event: end\n')
    } catch (_) {}
    try {
      res.write('data: {}\n\n')
    } catch (_) {}
    try {
      res.end()
    } catch (_) {}
  }
})

// Templates est√°ticos por modo/objetivo
const TemplatesQuery = z.object({ mode: z.enum(['SDR', 'Closer']).default('SDR') })
app.get('/templates', rateLimit('general'), (req, res) => {
  const mode = (req.query?.mode || 'SDR') === 'Closer' ? 'Closer' : 'SDR'
  const data = {
    SDR: {
      qualificar: [
        'Pergunte sobre or√ßamento, autoridade e necessidade do lead.',
        'Solicite disponibilidade para uma call de 15 minutos nesta semana.',
      ],
      obje√ß√µes: ['Responda a obje√ß√µes comuns sobre pre√ßo com foco em ROI e caso de uso.'],
    },
    Closer: {
      descoberta: ['Aprofunde nos requisitos t√©cnicos e restri√ß√µes de prazo.'],
      fechamento: ['Reforce valor e pr√≥ximos passos: contrato e cronograma de implementa√ß√£o.'],
    },
  }
  res.json({ mode, templates: data[mode] })
})

// Feedback üëçüëé
const FeedbackSchema = z.object({
  rating: z.enum(['up', 'down']),
  reason: z.string().max(500).optional(),
})
app.post('/feedback', rateLimit('general'), (req, res) => {
  const parsed = FeedbackSchema.safeParse(req.body)
  if (!parsed.success) {
    res.status(400).json({ code: 'BAD_REQUEST', message: 'Payload inv√°lido' })
    return
  }
  req.log?.info({ feedback: parsed.data }, 'feedback received')
  res.json({ ok: true })
})

const port = config.port
if (process.env.NODE_ENV !== 'test') {
  app.listen(port, () => {
    logger.info({ port }, 'Backend listening')
  })
}

export default app

// Diagn√≥stico de capacidade de streaming do modelo preferido
app.get('/diagnostics/llm', rateLimit('general'), async (req, res) => {
  const preferredModel = config.openai.modelPreferred || config.openai.model
  const fallbackModel = config.openai.modelFallback || config.openai.fallbackModel

  if (!config.openai.apiKey) {
    return res.json({
      preferredModel,
      fallbackModel,
      canStream: false,
      recommendation: 'configure api key',
      details: 'OPENAI_API_KEY n√£o configurada',
    })
  }

  // Helper local para detectar erro de restri√ß√£o de streaming
  function isStreamRestrictedError(err) {
    if (!err || err.code !== ERROR_CODES.UPSTREAM_ERROR || !err.details) return false
    const d = String(err.details)
    if (d.toLowerCase().includes('must be verified to stream this model')) return true
    if (d.toLowerCase().includes('verify organization')) return true
    try {
      const json = JSON.parse(d)
      const param = json?.error?.param
      const code = json?.error?.code
      if (param === 'stream' && code === 'unsupported_value') return true
    } catch (_) {
      /* noop */
    }
    return false
  }

  const controller = new AbortController()
  const timeoutId = setTimeout(() => {
    try {
      controller.abort(new Error('Timeout'))
    } catch (_) {}
  }, 8000)

  const startAt = Date.now()
  try {
    const body = {
      model: preferredModel,
      stream: true,
      messages: [
        { role: 'system', content: 'diagnostics' },
        { role: 'user', content: 'ping' },
      ],
      max_tokens: 1,
      temperature: 1,
    }

    const r = await fetch(`${config.openai.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${config.openai.apiKey}`,
      },
      body: JSON.stringify(body),
      signal: controller.signal,
    })

    if (!r.ok) {
      let text
      try {
        text = await r.text()
      } catch (_) {
        text = ''
      }
      const error = new Error(`OpenAI API Error: ${r.status}`)
      error.status = r.status
      error.details = text
      const mapped = mapUpstreamError(error, 'OpenAI')
      const restricted = isStreamRestrictedError(mapped)
      if ((process.env.LOG_LEVEL || 'info') === 'debug' && process.env.NODE_ENV !== 'production') {
        req.log?.info(
          { status: r.status, restricted, elapsedMs: Date.now() - startAt },
          'diagnostics llm error',
        )
      }
      return res.json({
        preferredModel,
        fallbackModel,
        canStream: false,
        recommendation: restricted ? 'fallback' : 'investigate',
        errorCode: mapped.code,
        details: restricted
          ? 'Streaming n√£o permitido para o modelo preferido nesta organiza√ß√£o'
          : mapped.message,
      })
    }

    const ctype = typeof r.headers?.get === 'function' ? r.headers.get('content-type') || '' : ''
    const isSSE = ctype.includes('text/event-stream')

    try {
      controller.abort(new Error('done'))
    } catch (_) {}

    if ((process.env.LOG_LEVEL || 'info') === 'debug' && process.env.NODE_ENV !== 'production') {
      req.log?.info({ isSSE, elapsedMs: Date.now() - startAt }, 'diagnostics llm success')
    }

    return res.json({
      preferredModel,
      fallbackModel,
      canStream: !!isSSE,
      recommendation: isSSE ? 'use preferred' : 'investigate',
    })
  } catch (e) {
    const mapped = mapUpstreamError(e, 'OpenAI')
    if ((process.env.LOG_LEVEL || 'info') === 'debug' && process.env.NODE_ENV !== 'production') {
      req.log?.warn(
        { err: mapped, elapsedMs: Date.now() - startAt },
        'diagnostics llm caught error',
      )
    }
    return res.json({
      preferredModel,
      fallbackModel,
      canStream: false,
      recommendation: 'investigate',
      errorCode: mapped.code,
      details: mapped.message,
    })
  } finally {
    clearTimeout(timeoutId)
  }
})

// Schema para cria√ß√£o de transcri√ß√£o
const TranscriptionCreateSchema = z.object({
  audio_url: z.string().url().or(z.string().min(1)), // aceitar url p√∫blica; valida√ß√£o simples
  speaker_labels: z.boolean().optional(),
  language_code: z.string().optional(),
})

app.post('/transcriptions', rateLimit('general'), async (req, res) => {
  const body = req.body || {}
  // Suporte a nomes alternativos de campo
  const payload = {
    audio_url: body.audio_url || body.audioUrl || body.url,
    speaker_labels: body.speaker_labels ?? true,
    language_code: body.language_code || 'pt',
  }
  const parsed = TranscriptionCreateSchema.safeParse(payload)
  if (!parsed.success) {
    return res.status(400).json({ code: 'BAD_REQUEST', message: 'Payload inv√°lido' })
  }
  if (!config.assembly.apiKey) {
    return res.status(500).json({ code: 'MISSING_API_KEY', message: 'ASSEMBLYAI_API_KEY n√£o configurada' })
  }
  try {
    const r = await fetch(`${config.assembly.baseUrl}/transcript`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: config.assembly.apiKey,
      },
      body: JSON.stringify({
        audio_url: parsed.data.audio_url,
        speaker_labels: parsed.data.speaker_labels ?? true,
        language_code: parsed.data.language_code || 'pt',
      }),
    })
    if (!r.ok) {
      const text = await r.text().catch(() => '')
      return res.status(502).json({ code: 'UPSTREAM_ERROR', message: 'Falha ao criar transcri√ß√£o', details: text })
    }
    const data = await r.json()
    return res.status(202).json({ id: data.id, status: data.status })
  } catch (err) {
    req.log?.error({ err }, 'transcriptions create error')
    return res.status(500).json({ code: 'INTERNAL_ERROR', message: 'Erro ao criar transcri√ß√£o' })
  }
})

app.post('/transcriptions/upload', rateLimit('general'), async (req, res) => {
  if (!config.assembly.apiKey) {
    return res
      .status(500)
      .json({ code: 'MISSING_API_KEY', message: 'ASSEMBLYAI_API_KEY n√£o configurada' })
  }
  try {
    // Encaminha o corpo bruto para o endpoint de upload da AssemblyAI
    const uploadRes = await fetch(`${config.assembly.baseUrl}/upload`, {
      method: 'POST',
      headers: {
        Authorization: config.assembly.apiKey,
        'Content-Type': req.headers['content-type'] || 'application/octet-stream',
      },
      body: req,
      duplex: 'half',
    })
    if (!uploadRes.ok) {
      const text = await uploadRes.text().catch(() => '')
      return res
        .status(502)
        .json({ code: 'UPSTREAM_ERROR', message: 'Falha ao enviar arquivo', details: text })
    }
    const uploadJson = await uploadRes.json().catch(() => ({}))
    const uploadUrl = uploadJson?.upload_url || uploadJson?.uploadUrl
    if (!uploadUrl) {
      return res
        .status(502)
        .json({ code: 'UPSTREAM_ERROR', message: 'Resposta inv√°lida do upload' })
    }

    const r = await fetch(`${config.assembly.baseUrl}/transcript`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: config.assembly.apiKey,
      },
      body: JSON.stringify({
        audio_url: uploadUrl,
        speaker_labels: true,
        language_code: 'pt',
      }),
    })
    if (!r.ok) {
      const text = await r.text().catch(() => '')
      return res
        .status(502)
        .json({ code: 'UPSTREAM_ERROR', message: 'Falha ao criar transcri√ß√£o', details: text })
    }
    const data = await r.json()
    return res.status(202).json({ id: data.id, status: data.status })
  } catch (err) {
    req.log?.error({ err }, 'transcriptions upload error')
    return res.status(500).json({ code: 'INTERNAL_ERROR', message: 'Erro ao processar upload' })
  }
})

app.get('/transcriptions/:id', rateLimit('general'), async (req, res) => {
  const id = req.params.id
  if (!id) return res.status(400).json({ code: 'BAD_REQUEST', message: 'ID ausente' })
  if (!config.assembly.apiKey) {
    return res.status(500).json({ code: 'MISSING_API_KEY', message: 'ASSEMBLYAI_API_KEY n√£o configurada' })
  }
  try {
    const r = await fetch(`${config.assembly.baseUrl}/transcript/${id}`, {
      headers: {
        Authorization: config.assembly.apiKey,
      },
    })
    if (!r.ok) {
      const text = await r.text().catch(() => '')
      return res.status(502).json({ code: 'UPSTREAM_ERROR', message: 'Falha ao obter transcri√ß√£o', details: text })
    }
    const data = await r.json()
    // Reduzir payload para a UI
    const out = {
      id: data.id,
      status: data.status,
      text: data.text,
      utterances: data.utterances,
      language_code: data.language_code,
      error: data.error,
    }
    return res.json(out)
  } catch (err) {
    req.log?.error({ err }, 'transcriptions get error')
    return res.status(500).json({ code: 'INTERNAL_ERROR', message: 'Erro ao consultar transcri√ß√£o' })
  }
})